# 人工智能系列：大语言模型的前世今生

## NLP 早期发展（1950-1990s）

### 符号方法与规则系统
早期的 NLP 系统主要基于符号方法和规则系统，例如上下文无关文法（Context-Free Grammar, CFG）。一个上下文无关文法是可以表示为一个四元组：

$$
G = \langle V, \Sigma, R, S \rangle
$$

其中
- $V$ 表示非终结符集合；
- $\Sigma$ 表示终结符集合；
- $R$ 表示产生式规则集合，形如 $A \to \alpha（其中 A \in V，\alpha 为 V \cup \Sigma 上的字符串）$；
- $S$ 是开始符号。

在计算机科学中，如果一个形式文法 G 的产生式规则都取自 R 的形式则称之为上下文无关文法。上下文无关文法取名为“上下文无关”就是因为字符 A 总可以被字符串 α 自由替换，而无需考虑字符 A 出现的上下文。如果一个形式语言是由上下文无关文法生成的，那么可以说这个形式语言是上下文无关的。例如，一个简单的上下文无关文法的例子是： $S \to aSb | ab$ ，这个文法就产生了语言 $\{a^nb^n | n \ge 1\}$。

上下文无关文法可以用来描述自然语言的句法结构，但是它很难处理自然语言中的歧义和复杂性。

### 统计语言模型（Statistical Language Model, SLM）（1990s）
统计语言模型是一种基于统计学习的方法，它通过统计分析文本数据中的词汇、句法和语义信息，来学习自然语言的规律。统计语言模型的目标是计算一个句子的概率，即 $P(w_1, w_2, \cdots, w_n)$，其中 $w_1, w_2, \cdots, w_n$ 是句子中的词汇。

#### N元语言模型（N-Grams） [^1][^2][^3]
该模型基于这样一种假设，第 $m$ 个词的出现只与前面 $m-1$ 个词相关，而与其他任何词都不相关。因此，根据链式规则，一个由 $m$ 个词组成的序列（句子）的概率：

$$
p(w_{1},w_{2},...,w_{m})=p(w_{1})*p(w_{2}|w_{1})*p(w_{3}|w_{1},w_{2})...p(w_{m}|w_{1},w_{2},...,w_{m-1})
$$

利用马尔科夫假设（Markov Assumption，即 $P(w_{m}|w_{1:m−1}) \approx P(w_{m}|w_{m−1})$），化简:

$$
p(w_{1},w_{2},...,w_{m})=\prod_{i=1}^m p(w_{i}|w_{i-n+1},...,w_{i-1})
$$

当 $n=2$，二元模型（bigram model）即为：

$$
p(w_{1},w_{2},...,w_{m})=\prod_{i=1}^m p(w_{i}|w_{i-1})
$$

当 $n=3$ ，三元模型（trigram model）即为：

$$
p(w_{1},w_{2},...,w_{m})=\prod_{i=1}^m p(w_{i}|w_{i-2}，w_{i-1})
$$

#### 隐马尔科夫模型（Hidden Markov Model, HMM）[^4]
HMM 通过观测序列推断隐藏状态序列，用于序列标注任务，其联合概率定义为：

$$
P(S_1, \ldots, S_T,\, O_1, \ldots, O_T) = P(S_1)P(O_1\mid S_1) \prod_{t=2}^{T} P(S_t\mid S_{t-1})P(O_t\mid S_t)
$$

其中
- $S_{t}$ 表示隐藏状态（如词性标签）
- $O_{t}$ 表示观测（如单词）

统计语言模型从大量数据中统计条件概率，自动从数据中提取语言模式，改善了机器翻译、语音识别等任务的性能，弥补了规则系统在处理数据多样性方面的不足。

## 神经网络崛起（2000s-2010s）
### 分布式表示（Word Embedding）
传统的 NLP 方法多采用 one-hot 编码表示词汇，这种表示方式维度高且无法捕捉词与词之间的语义关系。分布式表示（或词嵌入）通过将词映射到低维连续向量空间，使得语义上相似的词在向量空间中距离更近，从而捕捉词汇之间的语义和句法信息。

其中较为著名的方法有 Word2Vec[^5] 和 GloVe[^6]。以 Word2Vec 的 Skip-Gram 模型为例，其目标是最大化给定中心词预测其上下文词的概率，公式为：

$$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \le j \le c,; j \ne 0} \log, p(w_{t+j} \mid w_t)
$$

其中， $T$ 表示句子长度， $c$ 为上下文窗口大小，概率 $p(w_{O}|w_I)$ 通常采用 softmax 表达为：

$$
p(w_O \mid w_I) = \frac{\exp\left( v_{w_O}^\top v_{w_I} \right)}{\sum_{w \in V} \exp\left( v_w^\top v_{w_I} \right)}
$$

通过大量数据的训练，词嵌入不仅大幅降低了维度，也使得语义信息得以在向量空间中进行高效捕捉。

### 循环神经网络（RNN/LSTM）
### Seq2Seq与注意力机制（2014-2017）

## 预训练时代（2017-2020）
### Transformer架构（2017）
### 预训练范式突破（2018-2019）
#### BERT（Devlin et al., 2018）
#### GPT（Radford et al., 2018-2019）

## 大语言模型时代（2020至今）
### 模型规模化（Scaling Laws）
### 稀疏专家模型（MoE）
### 对齐与指令微调

## 参考文献
[^1]: Kuhn, T. S., & Morris, R. N. (1964). Statistical grammar and phrase recognition. IEEE Transactions on Communication Theory, 12(3), 333-345.
[^2]: Shannon, C. E. (1975). Prediction and entropy of printed English. MIT Press.
[^3]: Ney, H., Martin, A., & Kübler, N. (1975). A statistical interpretation of text-generating devices with application to the problem of speech recognition. IEEE Transactions on Information Theory, 21(6), 624-630.
[^4]: Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. The annals of mathematical statistics, 41(1), 164-171.
[^5]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.
[^6]: Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. EMNLP.
