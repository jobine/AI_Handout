# 人工智能系列：大语言模型的前世今生

## NLP 早期发展（1950-1990s）

### 符号方法与规则系统
早期的 NLP 系统主要基于符号方法和规则系统，例如上下文无关文法（Context-Free Grammar, CFG）。一个上下文无关文法是可以表示为一个四元组：

$$
G = \langle V, \Sigma, R, S \rangle
$$

其中
- $V$ 表示非终结符集合；
- $\Sigma$ 表示终结符集合；
- $R$ 表示产生式规则集合，形如 $A \to \alpha（其中 A \in V，\alpha 为 V \cup \Sigma 上的字符串）$；
- $S$ 是开始符号。

在计算机科学中，如果一个形式文法 G 的产生式规则都取自 R 的形式则称之为上下文无关文法。上下文无关文法取名为“上下文无关”就是因为字符 A 总可以被字符串 α 自由替换，而无需考虑字符 A 出现的上下文。如果一个形式语言是由上下文无关文法生成的，那么可以说这个形式语言是上下文无关的。例如，一个简单的上下文无关文法的例子是： $S \to aSb \mid ab$ ，这个文法就产生了语言 $\{a^nb^n \mid n \ge 1\}$。

上下文无关文法可以用来描述自然语言的句法结构，但是它很难处理自然语言中的歧义和复杂性。

#### 小结
这种形式化描述使我们能够构造诸如递归下降解析器等工具，以判断句子是否符合既定语法规则，并推导出相应的句法结构（即解析树）。尽管基于规则的方法在描述结构化、明确定义的语法方面具有明显优势，但它在捕捉自然语言的多样性与模糊性上存在局限，同时其扩展性也较为有限。

### 统计语言模型（Statistical Language Model, SLM）（1990s）
统计语言模型是一种基于统计学习的方法，它通过统计分析文本数据中的词汇、句法和语义信息，来学习自然语言的规律。 统计语言模型的目标是计算一个句子的概率，即 $P(w_1, w_2, \ldots, w_n)$，其中 $w_1, w_2, \ldots, w_n$ 是句子中的词汇。

#### N元语言模型（N-Grams） [^1][^2][^3]
该模型解决的问题是预测连续的 $m$ 个词汇组合及其出现概率。该模型基于这样一种假设，第 $m$ 个词的出现只与前面 $m-1$ 个词相关，而与其他任何词都不相关。那么，根据链式规则，一个由 $m$ 个词组成的序列（句子）的概率：

$$
p(w_{1},w_{2}, \ldots ,w_{m})=p(w_{1})*p(w_{2} \mid w_{1})*p(w_{3} \mid w_{1},w_{2}) \ldots p(w_{m} \mid w_{1:m-1})
$$

利用马尔科夫假设，即 

$$
P(w_{m} \mid w_{1:m−1}) = P(w_{m} \mid w_{m−1}) \approx P(w_m \mid w_{m-n+1:m-1}) \tag{Markov Assumption}
$$

化简:

$$
p(w_{1},w_{2}, \ldots ,w_{m})=\prod_{i=1}^m p(w_{i} \mid w_{i-n+1:i-1})
$$

当 $n=2$，二元模型（bigram model）即为：

$$
p(w_{1},w_{2}, \ldots ,w_{m})=\prod_{i=1}^m p(w_{i} \mid w_{i-1})
$$

当 $n=3$ ，三元模型（trigram model）即为：

$$
p(w_{1},w_{2}, \ldots ,w_{m})=\prod_{i=1}^m p(w_{i} \mid w_{i-2}，w_{i-1})
$$

#### 隐马尔科夫模型（Hidden Markov Model, HMM）[^4]
序列标注是自然语言处理中的一项重要任务，涉及为输入序列中的每个元素分配一个标签。常见的序列标注任务包括词性标注、命名实体识别和语义角色标注。HMM 通过观测序列推断隐藏状态序列，用于序列标注任务，其联合概率定义为：

$$
P(S_1, \ldots, S_T,\, O_1, \ldots, O_T) = P(S_1)P(O_1 \mid S_1) \prod_{t=2}^{T} P(S_t \mid S_{t-1})P(O_t \mid S_t)
$$

其中
- $S_{t}$ 表示隐藏状态（如词性标签）
- $O_{t}$ 表示观测（如单词）

一个 HMM 标注器（HMM Tagger）通常由两部分组成： $A$ 和 $B$ 两类概率，它们都是通过在带标注的训练语料上进行计数来估计得到的。

- $A$ 矩阵：包含词性转移概率 $P(t_i \mid t_{i-1})$，表示给定前一个词性标注的情况下，下一个词性标注出现的概率（例如一个 MD 之后紧跟一个 VB 的概率）。我们通过在标注语料中统计前一个标注出现的总次数，以及这两个标注连续出现的次数来计算该转移概率的最大似然估计：

$$
P(t_i \mid t_{i-1}) = \frac{C(t_{i-1},\,t_i)}{C(t_{i-1})}
$$

- $B$ 发射概率： $P(w_i \mid t_i)$ 表示给定一个词性（例如 MD）时，该词性会关联到某个具体单词（例如 “will”）的概率。发射概率的最大似然估计为：

$$
P(w_i \mid t_i) = \frac{C(t_i,\,w_i)}{C(t_i)}
$$

>对于任何包含隐藏变量的模型（例如 HMM），从观测序列中确定对应的隐藏变量序列的过程称为解码（decoding）:
>
>给定一个 HMM $\lambda = (A, B)$ 和一个观测序列 $O = o_1, o_2, \dots ,o_T$，找到最可能的状态序列 $S = s_1,s_2,s_3, \dots ,s_T$。

在词性标注（part-of-speech tagging）中，HMM 解码的目标是：给定包含 $m$ 个单词 $w_1, w_2, \dots, w_m$ 的观测序列，找到最可能的标注序列 $t_1, t_2, \dots, t_m$：

$$
\hat{t_{1:m}} = \arg\max_{t_1 \dots t_m} P(t_1 \dots t_m \mid w_1 \dots w_m) = \arg\max_{t_1 \dots t_m} \frac{P(w_1 \dots w_m \mid t_1 \dots t_m) \, P(t_1 \dots t_m)}{P(w_1 \dots w_m)}
$$

进一步忽略掉分母，可得：

$$
\hat{t_{1:m}} = \arg\max_{t_1 \dots t_m} P(w_1 \dots w_m \mid t_1 \dots t_m) \, P(t_1 \dots t_m)
$$

HMM 标注器还做了两个进一步的简化假设。

- 输出独立性：一个单词出现的概率只依赖于它自身的词性标注，而与邻近的单词和词性无关。

$$
P(o_i \mid s_{1:T}, o_{1:T}) = P(o_i \mid s_i) 
$$

- 马尔可夫假设：某个标注出现的概率只依赖于前一个标注，而不依赖于整个标注序列（参见公式 Markov Assumption）。

带入公式，最终得到二元标注器（bigram tagger）最可能的标注序列的计算公式：

$$
\hat{t_{1:m}} \approx \arg\max_{t_1 \dots t_m} \prod_{i=1}^{m} \overbrace{P(w_i \mid t_i)}^{发射概率} \, \overbrace{P(t_i \mid t_{i-1})}^{转移概率} 
$$

#### 小结
统计语言模型从大量数据中统计条件概率，自动从数据中提取语言模式，改善了机器翻译、语音识别等任务的性能，弥补了规则系统在处理数据多样性方面的不足。

## 神经网络崛起（2000s-2010s）
### 词嵌入（Word Embedding）
传统的 NLP 方法多采用 one-hot 编码表示词汇，这种表示方式维度高且无法捕捉词与词之间的语义关系。词嵌入（或分布式表示）通过将词映射到低维连续向量空间，使得语义上相似的词在向量空间中距离更近，从而捕捉词汇之间的语义和句法信息。

其中较为著名的方法有 Word2Vec[^5] 和 GloVe[^6]。以 Word2Vec 的 Skip-Gram 模型为例，其目标是最大化给定中心词预测其上下文词的概率，公式为：

$$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \le j \le c,; j \ne 0} \log, p(w_{t+j} \mid w_t)
$$

其中， $T$ 表示句子长度， $c$ 为上下文窗口大小，概率 $p(w_{O} \mid w_I)$ 通常采用 softmax 表达为：

$$
p(w_O \mid w_I) = \frac{\exp\left( v_{w_O}^\top v_{w_I} \right)}{\sum_{w \in V} \exp\left( v_w^\top v_{w_I} \right)}
$$

通过大量数据的训练，词嵌入不仅大幅降低了维度，也使得语义信息得以在向量空间中进行高效捕捉。

### 循环神经网络（RNN/LSTM）
### Seq2Seq与注意力机制（2014-2017）

## 预训练时代（2017-2020）
### Transformer架构（2017）
### 预训练范式突破（2018-2019）
#### BERT（Devlin et al., 2018）
#### GPT（Radford et al., 2018-2019）

## 大语言模型时代（2020至今）
### 模型规模化（Scaling Laws）
### 稀疏专家模型（MoE）
### 对齐与指令微调

## 参考文献
[^1]: Kuhn, T. S., & Morris, R. N. (1964). Statistical grammar and phrase recognition. IEEE Transactions on Communication Theory, 12(3), 333-345.
[^2]: Shannon, C. E. (1975). Prediction and entropy of printed English. MIT Press.
[^3]: Ney, H., Martin, A., & Kübler, N. (1975). A statistical interpretation of text-generating devices with application to the problem of speech recognition. IEEE Transactions on Information Theory, 21(6), 624-630.
[^4]: Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. The annals of mathematical statistics, 41(1), 164-171.
[^5]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.
[^6]: Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. EMNLP.
